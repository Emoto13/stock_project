{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_stock_forcasting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynvhMUgwuQiS"
      },
      "source": [
        "#!pip install imgaug==0.2.6 -q\n",
        "#!pip install PyYAML==5.3 -q\n",
        "#!pip install torchmetrics==0.2.0 -q\n",
        "#!pip install pytorch-lightning\n",
        "#!pip install lightning-flash -U"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgwB7h6A6t4K"
      },
      "source": [
        "n_steps = 3\n",
        "n_features = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ki-DBA853poQ",
        "outputId": "a5066cc7-43a2-422b-d35b-718ad52f8d6b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "msft_df = pd.read_csv('/content/drive/MyDrive/MSFT.csv')\n",
        "msft_df = msft_df.T\n",
        "msft_df.drop(['Unnamed: 0'], inplace=True)\n",
        "msft_df.drop(columns=[0,1,2,4], inplace= True)\n",
        "msft_df.rename(columns={3: 'y'}, inplace='True')\n",
        "msft_df=msft_df.iloc[::-1]\n",
        "msft_df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1999-11-12</th>\n",
              "      <td>89.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999-11-19</th>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999-11-26</th>\n",
              "      <td>91.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999-12-03</th>\n",
              "      <td>96.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999-12-10</th>\n",
              "      <td>93.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-26</th>\n",
              "      <td>236.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-01</th>\n",
              "      <td>242.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-09</th>\n",
              "      <td>255.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-16</th>\n",
              "      <td>260.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>260.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1120 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 y\n",
              "1999-11-12   89.19\n",
              "1999-11-19      86\n",
              "1999-11-26   91.12\n",
              "1999-12-03   96.12\n",
              "1999-12-10   93.87\n",
              "...            ...\n",
              "2021-03-26  236.48\n",
              "2021-04-01  242.35\n",
              "2021-04-09  255.85\n",
              "2021-04-16  260.74\n",
              "2021-04-21  260.58\n",
              "\n",
              "[1120 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em39UsPJQ9h7",
        "outputId": "a8970180-faf3-4d4f-c9d1-243f0e091d7b"
      },
      "source": [
        "msft_df.y = msft_df.y.astype('float32')\n",
        "msft_df.y.dtype"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EeYDr9z9VMW",
        "outputId": "57026117-4aba-4c98-9fa3-c47c4dad8475"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "X = msft_df.y.values[:-1]\n",
        "y = msft_df.y.values[1:]\n",
        "\n",
        "tscv = TimeSeriesSplit()\n",
        "for train_index, test_index in tscv.split(X):\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188] TEST: [189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206\n",
            " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224\n",
            " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
            " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
            " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296\n",
            " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
            " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
            " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
            " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
            " 369 370 371 372 373 374]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374] TEST: [375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
            " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
            " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428\n",
            " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
            " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
            " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
            " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
            " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
            " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
            " 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554\n",
            " 555 556 557 558 559 560]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560] TEST: [561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578\n",
            " 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596\n",
            " 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614\n",
            " 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632\n",
            " 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650\n",
            " 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668\n",
            " 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686\n",
            " 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704\n",
            " 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722\n",
            " 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740\n",
            " 741 742 743 744 745 746]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746] TEST: [747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764\n",
            " 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782\n",
            " 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800\n",
            " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818\n",
            " 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836\n",
            " 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854\n",
            " 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872\n",
            " 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890\n",
            " 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908\n",
            " 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926\n",
            " 927 928 929 930 931 932]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
            " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
            " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
            " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
            " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
            " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
            " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
            " 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n",
            " 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899\n",
            " 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917\n",
            " 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932] TEST: [ 933  934  935  936  937  938  939  940  941  942  943  944  945  946\n",
            "  947  948  949  950  951  952  953  954  955  956  957  958  959  960\n",
            "  961  962  963  964  965  966  967  968  969  970  971  972  973  974\n",
            "  975  976  977  978  979  980  981  982  983  984  985  986  987  988\n",
            "  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002\n",
            " 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016\n",
            " 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030\n",
            " 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044\n",
            " 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058\n",
            " 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072\n",
            " 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086\n",
            " 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100\n",
            " 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114\n",
            " 1115 1116 1117 1118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxqI0PlcCbqg",
        "outputId": "e96e8a42-724d-42bb-fb51-471c20901186"
      },
      "source": [
        "X_train, X_test\n",
        "y_train, y_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 86.    ,  91.12  ,  96.12  ,  93.87  , 115.25  , 117.44  ,\n",
              "        116.75  , 111.44  , 112.25  , 103.75  ,  98.25  , 106.56  ,\n",
              "         99.94  ,  95.06  ,  91.31  ,  96.12  , 101.    ,  99.37  ,\n",
              "        111.69  , 106.25  ,  89.06  ,  74.12  ,  78.94  ,  69.75  ,\n",
              "         71.12  ,  68.81  ,  65.06  ,  61.44  ,  66.31  ,  68.81  ,\n",
              "         72.56  ,  77.69  ,  80.    ,  82.    ,  78.94  ,  72.31  ,\n",
              "         69.69  ,  69.12  ,  72.44  ,  71.    ,  70.62  ,  70.19  ,\n",
              "         69.31  ,  64.19  ,  63.25  ,  60.31  ,  55.56  ,  53.75  ,\n",
              "         65.19  ,  67.69  ,  68.25  ,  67.37  ,  69.06  ,  69.94  ,\n",
              "         56.63  ,  54.44  ,  49.19  ,  46.44  ,  43.38  ,  49.13  ,\n",
              "         53.5   ,  61.    ,  64.    ,  60.81  ,  59.13  ,  57.31  ,\n",
              "         56.75  ,  56.69  ,  56.69  ,  54.56  ,  56.56  ,  54.69  ,\n",
              "         56.19  ,  62.18  ,  69.    ,  67.12  ,  70.75  ,  69.4   ,\n",
              "         68.09  ,  70.91  ,  70.34  ,  73.19  ,  68.02  ,  68.83  ,\n",
              "         73.    ,  66.06  ,  71.34  ,  69.18  ,  65.47  ,  66.89  ,\n",
              "         65.52  ,  61.88  ,  62.05  ,  57.05  ,  55.4   ,  57.58  ,\n",
              "         49.71  ,  51.17  ,  57.72  ,  56.38  ,  57.9   ,  62.2   ,\n",
              "         61.4   ,  65.21  ,  65.75  ,  64.71  ,  64.21  ,  67.83  ,\n",
              "         67.44  ,  67.54  ,  67.87  ,  68.9   ,  68.61  ,  66.1   ,\n",
              "         63.8   ,  62.66  ,  60.65  ,  60.23  ,  57.99  ,  61.37  ,\n",
              "         63.95  ,  62.49  ,  60.45  ,  60.31  ,  55.87  ,  55.93  ,\n",
              "         57.2   ,  51.5   ,  49.56  ,  50.05  ,  56.03  ,  53.26  ,\n",
              "         50.91  ,  51.98  ,  55.25  ,  52.28  ,  54.7   ,  54.85  ,\n",
              "         51.86  ,  49.56  ,  45.35  ,  44.41  ,  48.12  ,  50.    ,\n",
              "         52.22  ,  49.08  ,  47.82  ,  47.91  ,  47.46  ,  45.25  ,\n",
              "         43.77  ,  48.87  ,  53.15  ,  52.68  ,  53.    ,  55.1   ,\n",
              "         56.69  ,  58.22  ,  57.68  ,  55.47  ,  52.5   ,  53.04  ,\n",
              "         52.97  ,  53.79  ,  55.92  ,  51.46  ,  49.85  ,  47.46  ,\n",
              "         46.58  ,  48.3   ,  24.63  ,  23.7   ,  23.56  ,  24.86  ,\n",
              "         26.57  ,  24.67  ,  25.09  ,  24.2   ,  25.5   ,  25.22  ,\n",
              "         26.1   ,  26.36  ,  25.57  ,  24.22  ,  24.61  ,  23.67  ,\n",
              "         24.65  ,  26.33  ,  25.63  ,  26.5   ,  27.31  ,  26.89  ,\n",
              "         26.89  ,  26.17  ,  25.58  ,  25.54  ,  26.22  ,  26.52  ,\n",
              "         28.38  ,  28.34  ,  29.96  ,  28.19  ,  29.08  ,  28.91  ,\n",
              "         28.93  ,  26.61  ,  26.14  ,  26.1   ,  25.5   ,  25.11  ,\n",
              "         25.71  ,  25.98  ,  26.65  ,  27.36  ,  27.21  ,  27.45  ,\n",
              "         27.66  ,  27.81  ,  28.48  ,  27.65  ,  27.08  ,  26.59  ,\n",
              "         26.57  ,  26.53  ,  26.35  ,  25.38  ,  24.63  ,  25.03  ,\n",
              "         25.85  ,  25.48  ,  25.16  ,  27.54  ,  26.13  ,  25.78  ,\n",
              "         25.86  ,  25.89  ,  26.23  ,  25.95  ,  26.77  ,  28.35  ,\n",
              "         28.57  ,  28.57  ,  27.86  ,  27.48  ,  28.03  ,  28.49  ,\n",
              "         27.14  ,  27.02  ,  27.2   ,  27.46  ,  27.11  ,  27.49  ,\n",
              "         27.51  ,  27.29  ,  28.25  ,  27.99  ,  27.99  ,  27.74  ,\n",
              "         27.97  ,  29.31  ,  29.97  ,  26.86  ,  26.6   ,  27.23  ,\n",
              "         27.08  ,  26.96  ,  27.01  ,  26.72  ,  26.67  ,  26.12  ,\n",
              "         25.65  ,  26.18  ,  26.32  ,  25.97  ,  25.48  ,  25.25  ,\n",
              "         25.17  ,  25.09  ,  24.31  ,  24.28  ,  24.12  ,  24.94  ,\n",
              "         24.46  ,  24.98  ,  25.3   ,  25.22  ,  25.3   ,  25.74  ,\n",
              "         26.07  ,  25.43  ,  25.43  ,  25.04  ,  25.04  ,  24.71  ,\n",
              "         25.09  ,  25.79  ,  25.68  ,  25.61  ,  27.76  ,  27.05  ,\n",
              "         26.72  ,  26.97  ,  27.02  ,  26.58  ,  26.07  ,  25.27  ,\n",
              "         25.73  ,  24.59  ,  24.67  ,  24.78  ,  25.53  ,  26.66  ,\n",
              "         27.28  ,  28.07  ,  27.76  ,  28.01  ,  27.71  ,  26.9   ,\n",
              "         26.64  ,  26.15  ,  26.91  ,  27.19  ,  26.41  ,  27.79  ,\n",
              "         27.54  ,  26.69  ,  26.7   ,  26.63  ,  26.93  ,  27.17  ,\n",
              "         27.5   ,  27.01  ,  27.21  ,  27.25  ,  27.07  ,  27.15  ,\n",
              "         24.15  ,  23.8   ,  23.17  ,  22.56  ,  23.72  ,  22.76  ,\n",
              "         22.03  ,  22.1   ,  22.5   ,  23.3   ,  23.3   ,  22.29  ,\n",
              "         23.87  ,  24.25  ,  24.29  ,  24.43  ,  25.79  ,  25.85  ,\n",
              "         25.84  ,  25.6   ,  26.85  ,  26.66  ,  27.35  ,  27.87  ,\n",
              "         28.37  ,  28.43  ,  28.34  ,  28.73  ,  29.24  ,  29.4   ,\n",
              "         29.76  ,  29.12  ,  29.4   ,  30.19  ,  29.64  ,  29.86  ,\n",
              "         29.64  ,  31.21  ,  31.11  ,  30.6   ,  30.19  ,  28.98  ,\n",
              "         28.74  ,  28.9   ,  27.76  ,  27.28  ,  27.33  ,  28.02  ,\n",
              "         27.87  ,  28.55  ,  28.61  ,  29.02  ,  30.12  ,  30.56  ,\n",
              "         30.89  ,  30.83  ,  30.48  ,  30.59  ,  30.05  ,  30.49  ,\n",
              "         29.49  ,  29.47  ,  29.97  ,  29.82  ,  31.16  ,  29.3901,\n",
              "         28.9601,  28.71  ,  28.25  ,  28.81  ,  28.73  ,  28.44  ,\n",
              "         29.04  ,  28.6499,  29.46  ,  29.84  ,  30.17  ,  30.17  ,\n",
              "         35.03  ,  37.06  ,  33.73  ,  34.09  ,  34.11  ,  33.6   ,\n",
              "         34.53  ,  35.31  ,  36.06  ,  36.12  ,  34.38  ,  33.91  ,\n",
              "         33.01  ,  32.94  ,  30.4498,  28.56  ,  28.42  ,  27.68  ,\n",
              "         27.1999,  27.87  ,  27.96  ,  29.18  ,  27.91  ,  29.16  ,\n",
              "         28.28  ,  30.    ,  29.83  ,  29.24  ,  29.39  ,  29.99  ,\n",
              "         28.05  ,  28.32  ,  27.49  ,  29.07  ,  28.23  ,  27.63  ,\n",
              "         25.98  ,  25.25  ,  25.86  ,  26.16  ,  25.44  ,  28.13  ,\n",
              "         27.81  ,  27.84  ,  27.29  ,  25.65  ,  27.62  ,  25.16  ,\n",
              "         27.4   ,  26.32  ,  21.5   ,  23.93  ,  21.96  ,  22.33  ,\n",
              "         21.5   ,  20.06  ,  19.68  ,  20.22  ,  19.87  ,  19.36  ,\n",
              "         19.12  ,  19.13  ,  20.33  ,  19.52  ,  19.71  ,  17.2   ,\n",
              "         17.1   ,  19.66  ,  19.09  ,  18.    ,  16.15  ,  15.28  ,\n",
              "         16.65  ,  17.06  ,  18.13  ,  18.75  ,  19.67  ,  19.2   ,\n",
              "         20.91  ,  20.24  ,  19.42  ,  20.22  ,  19.75  ,  20.89  ,\n",
              "         22.14  ,  23.33  ,  24.07  ,  23.35  ,  23.37  ,  22.39  ,\n",
              "         24.29  ,  23.45  ,  23.52  ,  23.56  ,  23.692 ,  24.41  ,\n",
              "         24.68  ,  24.62  ,  24.86  ,  25.26  ,  25.55  ,  24.96  ,\n",
              "         25.55  ,  26.5   ,  28.02  ,  27.73  ,  28.52  ,  29.63  ,\n",
              "         29.62  ,  29.22  ,  29.98  ,  29.85  ,  30.36  ,  31.    ,\n",
              "         30.48  ,  30.66  ,  30.86  ,  28.96  ,  28.18  ,  28.02  ,\n",
              "         27.93  ,  28.77  ,  28.67  ,  28.5875,  29.27  ,  29.59  ,\n",
              "         29.66  ,  29.16  ,  30.34  ,  30.67  ,  30.96  ,  30.535 ,\n",
              "         28.21  ,  28.93  ,  26.84  ,  25.8   ,  25.79  ,  25.66  ,\n",
              "         26.44  ,  24.5325,  23.27  ,  24.27  ,  24.89  ,  25.81  ,\n",
              "         25.81  ,  25.55  ,  24.4   ,  24.23  ,  23.93  ,  24.29  ,\n",
              "         23.85  ,  25.22  ,  24.775 ,  24.38  ,  24.57  ,  25.54  ,\n",
              "         25.3775,  26.665 ,  26.85  ,  26.27  ,  25.69  ,  25.25  ,\n",
              "         27.02  ,  27.34  ,  27.9025,  28.3   ,  27.91  ,  28.6   ,\n",
              "         28.3   ,  28.02  ,  27.751 ,  27.77  ,  27.25  ,  27.06  ,\n",
              "         26.55  ,  25.9525,  25.68  ,  24.8   ,  25.62  ,  25.48  ,\n",
              "         26.07  ,  25.37  ,  25.52  ,  25.92  ,  25.87  ,  25.03  ,\n",
              "         24.49  ,  24.76  ,  23.905 ,  23.705 ,  24.26  ,  24.3   ,\n",
              "         26.02  ,  26.92  ,  26.78  ,  27.53  ,  27.4   ,  25.68  ,\n",
              "         25.1   ,  24.05  ,  25.25  ,  25.8   ,  25.74  ,  27.12  ,\n",
              "         25.06  ,  24.89  ,  26.25  ,  27.27  ,  27.16  ,  26.98  ,\n",
              "         26.25  ,  26.91  ,  25.3   ,  24.3   ,  25.22  ,  25.7   ,\n",
              "         26.    ,  26.03  ,  25.96  ,  28.105 ,  28.25  ,  29.71  ,\n",
              "         29.23  ,  30.24  ,  30.495 ,  31.25  ,  31.48  ,  32.075 ,\n",
              "         31.99  ,  32.6   ,  32.01  ,  32.255 ,  31.52  ,  30.81  ,\n",
              "         32.42  ,  31.98  ,  30.98  ,  31.16  ,  29.27  ,  29.06  ,\n",
              "         28.45  ,  29.65  ,  30.02  ,  30.7   ,  30.59  ,  30.185 ,\n",
              "         29.39  ,  30.115 ,  29.755 ,  29.75  ,  30.42  ,  30.9   ,\n",
              "         30.56  ,  30.82  ,  30.95  ,  31.21  ,  31.19  ,  29.76  ,\n",
              "         29.85  ,  29.2   ,  28.64  ,  28.21  ,  29.4998,  28.83  ,\n",
              "         26.5209,  27.7   ,  26.615 ,  26.455 ,  26.81  ,  27.45  ,\n",
              "         26.55  ,  26.74  ,  26.83  ,  27.25  ,  27.88  ,  27.93  ,\n",
              "         27.55  ,  28.01  ,  27.76  ,  27.95  ,  28.    ,  28.035 ,\n",
              "         28.25  ,  28.605 ,  28.7   ,  28.79  ,  29.765 ,  31.79  ,\n",
              "         33.49  ,  32.69  ,  34.87  ,  34.269 ,  34.9   ,  35.67  ,\n",
              "         34.4   ,  33.265 ,  34.545 ,  34.21  ,  35.67  ,  31.4   ,\n",
              "         31.62  ,  31.89  ,  32.7   ,  31.8   ,  34.75  ,  33.4   ,\n",
              "         31.152 ,  33.03  ,  32.791 ,  33.27  ,  33.88  ,  34.13  ,\n",
              "         34.96  ,  35.73  ,  35.525 ,  37.78  ,  37.841 ,  37.57  ,\n",
              "         38.13  ,  38.36  ,  36.69  ,  36.8   ,  37.29  ,  36.91  ,\n",
              "         36.04  ,  36.38  ,  36.805 ,  37.84  ,  36.56  ,  37.62  ,\n",
              "         37.98  ,  38.31  ,  37.9   ,  37.7   ,  40.16  ,  40.3   ,\n",
              "         39.87  ,  39.209 ,  40.01  ,  39.91  ,  39.69  ,  39.54  ,\n",
              "         39.83  ,  40.12  ,  40.94  ,  41.48  ,  41.23  ,  41.68  ,\n",
              "         42.25  ,  41.8   ,  42.09  ,  44.69  ,  44.5   ,  42.86  ,\n",
              "         43.2   ,  44.79  ,  45.15  ,  45.43  ,  45.91  ,  46.695 ,\n",
              "         47.52  ,  46.41  ,  46.09  ,  44.03  ,  43.63  ,  46.13  ,\n",
              "         46.95  ,  48.68  ,  49.58  ,  47.98  ,  47.81  ,  48.42  ,\n",
              "         46.95  ,  47.66  ,  47.88  ,  46.76  ,  47.19  ,  46.24  ,\n",
              "         47.18  ,  40.4   ,  42.41  ,  43.87  ,  43.855 ,  43.85  ,\n",
              "         42.36  ,  41.38  ,  42.88  ,  40.97  ,  40.29  ,  41.72  ,\n",
              "         41.615 ,  47.87  ,  48.655 ,  47.75  ,  48.295 ,  46.9   ,\n",
              "         46.86  ,  46.14  ,  45.97  ,  46.1   ,  45.26  ,  44.4   ,\n",
              "         44.61  ,  46.62  ,  45.94  ,  46.7   ,  46.74  ,  47.    ,\n",
              "         43.07  ,  43.93  ,  42.61  ,  43.48  ,  43.48  ,  43.94  ,\n",
              "         45.57  ,  47.11  ,  47.51  ,  52.87  ,  52.64  ,  54.92  ,\n",
              "         52.84  ,  54.19  ,  53.93  ,  55.91  ,  54.06  ,  54.13  ,\n",
              "         55.67  ,  55.48  ,  52.33  ,  50.99  ,  52.29  ,  55.09  ,\n",
              "         50.16  ,  50.5   ,  51.82  ,  51.3   ,  52.03  ,  53.07  ,\n",
              "         53.49  ,  54.21  ,  55.57  ,  54.42  ,  55.65  ,  51.78  ,\n",
              "         49.87  ,  50.39  ,  51.08  ,  50.62  ,  52.32  ,  51.79  ,\n",
              "         51.48  ,  50.13  ,  49.83  ,  51.16  ,  52.3   ,  53.7   ,\n",
              "         56.57  ,  56.68  ,  57.96  ,  57.94  ,  57.62  ,  58.03  ,\n",
              "         57.67  ,  56.21  ,  57.25  ,  57.43  ,  57.6   ,  57.8   ,\n",
              "         57.42  ,  59.66  ,  59.87  ,  58.71  ,  59.02  ,  60.35  ,\n",
              "         60.53  ,  59.25  ,  61.97  ,  62.3   ,  63.24  ,  62.14  ,\n",
              "         62.84  ,  62.7   ,  62.74  ,  65.78  ,  63.68  ,  64.    ,\n",
              "         64.62  ,  64.62  ,  64.25  ,  64.93  ,  64.87  ,  64.98  ,\n",
              "         65.86  ,  65.68  ,  64.95  ,  66.4   ,  68.46  ,  69.    ,\n",
              "         68.38  ,  67.69  ,  69.96  ,  71.76  ,  70.32  ,  70.    ,\n",
              "         71.21  ,  68.93  ,  69.46  ,  72.78  ,  73.79  ,  73.04  ,\n",
              "         72.68  ,  72.5   ,  72.49  ,  72.82  ,  73.94  ,  73.98  ,\n",
              "         75.31  ,  74.41  ,  74.49  ], dtype=float32),\n",
              " array([ 76.  ,  77.49,  78.81,  83.81,  84.14,  83.87,  82.4 ,  83.26,\n",
              "         84.26,  84.16,  86.85,  85.51,  85.54,  88.19,  89.6 ,  90.  ,\n",
              "         94.06,  91.78,  88.18,  92.  ,  94.06,  93.05,  96.54,  94.6 ,\n",
              "         87.18,  91.27,  90.23,  93.08,  95.  ,  95.82,  95.16,  97.7 ,\n",
              "         96.36,  98.36, 100.79, 101.63, 100.13, 100.41,  98.61, 101.16,\n",
              "        105.43, 106.27, 107.68, 108.04, 109.  , 107.58, 108.4 , 112.33,\n",
              "        108.21, 113.37, 114.26, 114.37, 112.13, 109.57, 108.66, 106.96,\n",
              "        106.16, 109.57, 108.29, 103.07, 110.89, 104.82, 106.03,  98.23,\n",
              "        100.39, 101.93, 102.8 , 107.71, 107.17, 102.78, 105.67, 108.22,\n",
              "        110.97, 112.53, 110.51, 115.91, 117.05, 117.94, 119.89, 120.95,\n",
              "        123.37, 129.89, 128.9 , 127.13, 128.07, 126.24, 123.68, 131.4 ,\n",
              "        132.45, 136.97, 133.96, 137.06, 138.9 , 136.62, 141.34, 136.9 ,\n",
              "        137.71, 136.13, 133.39, 137.86, 139.1 , 137.32, 139.44, 137.73,\n",
              "        138.12, 139.68, 137.41, 140.73, 143.72, 145.96, 149.97, 149.59,\n",
              "        151.38, 151.75, 154.53, 157.41, 158.96, 158.62, 161.34, 167.1 ,\n",
              "        165.04, 170.23, 183.89, 185.35, 178.59, 162.01, 161.57, 158.83,\n",
              "        137.35, 149.7 , 153.83, 165.14, 178.6 , 174.55, 174.57, 184.68,\n",
              "        183.16, 183.51, 183.25, 187.2 , 187.74, 195.15, 196.33, 206.26,\n",
              "        213.67, 202.88, 201.3 , 205.01, 212.48, 208.9 , 213.02, 228.91,\n",
              "        214.25, 204.03, 200.39, 207.82, 206.19, 215.81, 219.66, 216.23,\n",
              "        202.47, 223.72, 216.51, 210.39, 215.23, 214.36, 213.26, 218.59,\n",
              "        222.75, 222.42, 219.62, 212.65, 225.95, 231.96, 242.2 , 244.99,\n",
              "        240.97, 232.38, 231.6 , 235.75, 230.35, 236.48, 242.35, 255.85,\n",
              "        260.74, 260.58], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thwIVDAnO2T3"
      },
      "source": [
        "import numpy as np\n",
        "def prepare_data(timeseries_data, n_features):\n",
        "\tX, y =[],[]\n",
        "\tfor i in range(len(timeseries_data)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_features\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(timeseries_data)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x = timeseries_data[i:end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\treturn np.array(X)\n",
        "\n",
        "X_train = prepare_data(X_train, n_steps)\n",
        "X_test = prepare_data(X_test, n_steps) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY9-2FlqPw2-"
      },
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEQ1lD_uyEn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22a7e2f-29a7-4e7c-cfd6-0f0cffeb396e"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/TF_stock_forecaster.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "# fit model\n",
        "\n",
        "model.fit(X_train, y_train, epochs=300, batch_size=64, verbose=1, callbacks=[cp_callback], validation_split = 0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "14/14 [==============================] - 3s 53ms/step - loss: 36.3039 - val_loss: 49.8850\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 49.88502, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 2/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 28.7069 - val_loss: 19.5304\n",
            "\n",
            "Epoch 00002: val_loss improved from 49.88502 to 19.53037, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 3/300\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 7.2480 - val_loss: 3.9383\n",
            "\n",
            "Epoch 00003: val_loss improved from 19.53037 to 3.93829, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 4/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7955 - val_loss: 0.8789\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.93829 to 0.87888, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 5/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.9552 - val_loss: 0.7684\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.87888 to 0.76844, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 6/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8980 - val_loss: 1.0232\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.76844\n",
            "Epoch 7/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8416 - val_loss: 0.7842\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.76844\n",
            "Epoch 8/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8096 - val_loss: 0.7164\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76844 to 0.71636, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 9/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8282 - val_loss: 1.0456\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.71636\n",
            "Epoch 10/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.9305 - val_loss: 0.6499\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.71636 to 0.64995, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 11/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8163 - val_loss: 0.6279\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.64995 to 0.62792, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 12/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7606 - val_loss: 0.6192\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.62792 to 0.61917, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 13/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8046 - val_loss: 0.6217\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.61917\n",
            "Epoch 14/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7986 - val_loss: 0.8273\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.61917\n",
            "Epoch 15/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8087 - val_loss: 0.6453\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.61917\n",
            "Epoch 16/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7795 - val_loss: 0.6339\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.61917\n",
            "Epoch 17/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7660 - val_loss: 0.6122\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.61917 to 0.61223, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 18/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.7484 - val_loss: 0.8596\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.61223\n",
            "Epoch 19/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8835 - val_loss: 0.7479\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.61223\n",
            "Epoch 20/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7101 - val_loss: 0.8624\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.61223\n",
            "Epoch 21/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8272 - val_loss: 0.7542\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.61223\n",
            "Epoch 22/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8608 - val_loss: 0.7761\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.61223\n",
            "Epoch 23/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7737 - val_loss: 0.8447\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.61223\n",
            "Epoch 24/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8161 - val_loss: 0.7196\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.61223\n",
            "Epoch 25/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7620 - val_loss: 0.7086\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.61223\n",
            "Epoch 26/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7236 - val_loss: 0.9696\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.61223\n",
            "Epoch 27/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7892 - val_loss: 0.5956\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.61223 to 0.59562, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 28/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7132 - val_loss: 0.7198\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.59562\n",
            "Epoch 29/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7467 - val_loss: 0.6388\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.59562\n",
            "Epoch 30/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7516 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.59562\n",
            "Epoch 31/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6723 - val_loss: 0.9980\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.59562\n",
            "Epoch 32/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8411 - val_loss: 0.9435\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.59562\n",
            "Epoch 33/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8184 - val_loss: 0.6348\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.59562\n",
            "Epoch 34/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8469 - val_loss: 0.7158\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.59562\n",
            "Epoch 35/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8301 - val_loss: 0.5850\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.59562 to 0.58498, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 36/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8311 - val_loss: 0.8494\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58498\n",
            "Epoch 37/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7294 - val_loss: 0.6075\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.58498\n",
            "Epoch 38/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6659 - val_loss: 0.6307\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.58498\n",
            "Epoch 39/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7256 - val_loss: 0.9864\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.58498\n",
            "Epoch 40/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7117 - val_loss: 1.1884\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.58498\n",
            "Epoch 41/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8456 - val_loss: 0.5630\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.58498 to 0.56298, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 42/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8427 - val_loss: 0.9311\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.56298\n",
            "Epoch 43/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.8283 - val_loss: 0.6774\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.56298\n",
            "Epoch 44/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7659 - val_loss: 0.6217\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.56298\n",
            "Epoch 45/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6511 - val_loss: 0.5734\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.56298\n",
            "Epoch 46/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6720 - val_loss: 0.5477\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.56298 to 0.54769, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 47/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6671 - val_loss: 0.7174\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.54769\n",
            "Epoch 48/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7169 - val_loss: 1.6879\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54769\n",
            "Epoch 49/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0548 - val_loss: 1.4559\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54769\n",
            "Epoch 50/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8639 - val_loss: 0.7400\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54769\n",
            "Epoch 51/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7652 - val_loss: 0.5781\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.54769\n",
            "Epoch 52/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7928 - val_loss: 0.7165\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.54769\n",
            "Epoch 53/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7285 - val_loss: 0.6678\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.54769\n",
            "Epoch 54/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6799 - val_loss: 0.7496\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.54769\n",
            "Epoch 55/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7193 - val_loss: 0.9759\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.54769\n",
            "Epoch 56/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7122 - val_loss: 0.5435\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.54769 to 0.54352, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 57/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6985 - val_loss: 0.7085\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54352\n",
            "Epoch 58/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6620 - val_loss: 0.8153\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54352\n",
            "Epoch 59/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6575 - val_loss: 1.7105\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54352\n",
            "Epoch 60/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8688 - val_loss: 0.5220\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.54352 to 0.52199, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 61/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6482 - val_loss: 0.7450\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.52199\n",
            "Epoch 62/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6260 - val_loss: 0.4998\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.52199 to 0.49981, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 63/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6087 - val_loss: 0.9575\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.49981\n",
            "Epoch 64/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6769 - val_loss: 0.4989\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.49981 to 0.49891, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 65/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6064 - val_loss: 0.6404\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.49891\n",
            "Epoch 66/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5352 - val_loss: 0.2745\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.49891 to 0.27451, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 67/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3893 - val_loss: 1.6312\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.27451\n",
            "Epoch 68/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5902 - val_loss: 0.5261\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.27451\n",
            "Epoch 69/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3948 - val_loss: 0.7869\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.27451\n",
            "Epoch 70/300\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.4655 - val_loss: 1.5327\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.27451\n",
            "Epoch 71/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.5671 - val_loss: 0.8853\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.27451\n",
            "Epoch 72/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3801 - val_loss: 0.0908\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.27451 to 0.09077, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 73/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2741 - val_loss: 0.4322\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.09077\n",
            "Epoch 74/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2868 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.09077 to 0.08444, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 75/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3796 - val_loss: 1.0081\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.08444\n",
            "Epoch 76/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5380 - val_loss: 0.5827\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.08444\n",
            "Epoch 77/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3081 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.08444 to 0.08223, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 78/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2032 - val_loss: 1.2173\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.08223\n",
            "Epoch 79/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3859 - val_loss: 0.2639\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.08223\n",
            "Epoch 80/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2742 - val_loss: 0.0403\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.08223 to 0.04027, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 81/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1812 - val_loss: 0.4357\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.04027\n",
            "Epoch 82/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2892 - val_loss: 1.4285\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.04027\n",
            "Epoch 83/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6497 - val_loss: 0.1883\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.04027\n",
            "Epoch 84/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2885 - val_loss: 0.1319\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.04027\n",
            "Epoch 85/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1731 - val_loss: 0.1857\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.04027\n",
            "Epoch 86/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1800 - val_loss: 0.5728\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.04027\n",
            "Epoch 87/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2550 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.04027\n",
            "Epoch 88/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2794 - val_loss: 0.3057\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.04027\n",
            "Epoch 89/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1907 - val_loss: 0.0995\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.04027\n",
            "Epoch 90/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2557 - val_loss: 0.8535\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04027\n",
            "Epoch 91/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3492 - val_loss: 0.2770\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04027\n",
            "Epoch 92/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2726 - val_loss: 0.2259\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04027\n",
            "Epoch 93/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1735 - val_loss: 1.3548\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.04027\n",
            "Epoch 94/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.5887 - val_loss: 1.1442\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04027\n",
            "Epoch 95/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5612 - val_loss: 0.5880\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04027\n",
            "Epoch 96/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2276 - val_loss: 0.0702\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.04027\n",
            "Epoch 97/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1498 - val_loss: 0.6440\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.04027\n",
            "Epoch 98/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2872 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.04027\n",
            "Epoch 99/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2555 - val_loss: 0.3518\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.04027\n",
            "Epoch 100/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1467 - val_loss: 0.3567\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.04027\n",
            "Epoch 101/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1806 - val_loss: 0.7544\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.04027\n",
            "Epoch 102/300\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.2965 - val_loss: 0.3437\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.04027\n",
            "Epoch 103/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2475 - val_loss: 0.2329\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.04027\n",
            "Epoch 104/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2491 - val_loss: 1.7906\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.04027\n",
            "Epoch 105/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8073 - val_loss: 0.1367\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.04027\n",
            "Epoch 106/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2619 - val_loss: 0.0407\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.04027\n",
            "Epoch 107/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1808 - val_loss: 0.5798\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.04027\n",
            "Epoch 108/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2268 - val_loss: 0.1035\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.04027\n",
            "Epoch 109/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1154 - val_loss: 0.3035\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.04027\n",
            "Epoch 110/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2522 - val_loss: 0.6221\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.04027\n",
            "Epoch 111/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2562 - val_loss: 0.4203\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.04027\n",
            "Epoch 112/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2419 - val_loss: 0.0380\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.04027 to 0.03796, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 113/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1229 - val_loss: 0.8871\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.03796\n",
            "Epoch 114/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4629 - val_loss: 0.2477\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.03796\n",
            "Epoch 115/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1834 - val_loss: 0.3229\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.03796\n",
            "Epoch 116/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1342 - val_loss: 0.3134\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.03796\n",
            "Epoch 117/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2194 - val_loss: 0.3523\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.03796\n",
            "Epoch 118/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2125 - val_loss: 0.0897\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.03796\n",
            "Epoch 119/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.3536\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.03796\n",
            "Epoch 120/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2371 - val_loss: 0.0593\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.03796\n",
            "Epoch 121/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1964 - val_loss: 0.1696\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.03796\n",
            "Epoch 122/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1898 - val_loss: 0.1196\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.03796\n",
            "Epoch 123/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2088 - val_loss: 0.1935\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.03796\n",
            "Epoch 124/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1857 - val_loss: 0.2616\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.03796\n",
            "Epoch 125/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3849 - val_loss: 0.2514\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.03796\n",
            "Epoch 126/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2633 - val_loss: 0.1193\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.03796\n",
            "Epoch 127/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2567 - val_loss: 0.8179\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.03796\n",
            "Epoch 128/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3739 - val_loss: 0.1473\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.03796\n",
            "Epoch 129/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2720 - val_loss: 0.1202\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.03796\n",
            "Epoch 130/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1744 - val_loss: 0.5007\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.03796\n",
            "Epoch 131/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2338 - val_loss: 0.1481\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.03796\n",
            "Epoch 132/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1914 - val_loss: 0.4868\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.03796\n",
            "Epoch 133/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2047 - val_loss: 0.0784\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.03796\n",
            "Epoch 134/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1707 - val_loss: 0.2302\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.03796\n",
            "Epoch 135/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1732 - val_loss: 0.6049\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.03796\n",
            "Epoch 136/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2876 - val_loss: 0.5083\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.03796\n",
            "Epoch 137/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3134 - val_loss: 0.3098\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.03796\n",
            "Epoch 138/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3095 - val_loss: 0.5131\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.03796\n",
            "Epoch 139/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2409 - val_loss: 0.2674\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.03796\n",
            "Epoch 140/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1136 - val_loss: 0.2078\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.03796\n",
            "Epoch 141/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1712 - val_loss: 0.4224\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.03796\n",
            "Epoch 142/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1698 - val_loss: 0.1612\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.03796\n",
            "Epoch 143/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1575 - val_loss: 0.3867\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.03796\n",
            "Epoch 144/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1096 - val_loss: 0.0895\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.03796\n",
            "Epoch 145/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2084 - val_loss: 0.7783\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.03796\n",
            "Epoch 146/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5219 - val_loss: 0.0415\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.03796\n",
            "Epoch 147/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0977 - val_loss: 0.2135\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.03796\n",
            "Epoch 148/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1039 - val_loss: 0.6742\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.03796\n",
            "Epoch 149/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4690 - val_loss: 0.3805\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.03796\n",
            "Epoch 150/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2688 - val_loss: 0.3024\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.03796\n",
            "Epoch 151/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1587 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.03796\n",
            "Epoch 152/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1555 - val_loss: 0.5643\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.03796\n",
            "Epoch 153/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2207 - val_loss: 0.0292\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.03796 to 0.02924, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 154/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0726 - val_loss: 0.0500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.02924\n",
            "Epoch 155/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1076 - val_loss: 0.4206\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.02924\n",
            "Epoch 156/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2284 - val_loss: 0.1856\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.02924\n",
            "Epoch 157/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1643 - val_loss: 0.1319\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.02924\n",
            "Epoch 158/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1162 - val_loss: 0.3992\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.02924\n",
            "Epoch 159/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2911 - val_loss: 0.0340\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.02924\n",
            "Epoch 160/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.02924\n",
            "Epoch 161/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1491 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.02924\n",
            "Epoch 162/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2086 - val_loss: 0.1648\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.02924\n",
            "Epoch 163/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1090 - val_loss: 0.0587\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.02924\n",
            "Epoch 164/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1012 - val_loss: 0.5236\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.02924\n",
            "Epoch 165/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1932 - val_loss: 0.0450\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.02924\n",
            "Epoch 166/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0790 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.02924\n",
            "Epoch 167/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3111 - val_loss: 0.3305\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.02924\n",
            "Epoch 168/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1565 - val_loss: 0.2189\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.02924\n",
            "Epoch 169/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1761 - val_loss: 0.6259\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.02924\n",
            "Epoch 170/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3602 - val_loss: 0.1859\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.02924\n",
            "Epoch 171/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2820 - val_loss: 0.7663\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.02924\n",
            "Epoch 172/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4557 - val_loss: 0.5718\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.02924\n",
            "Epoch 173/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2770 - val_loss: 0.4022\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.02924\n",
            "Epoch 174/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1630 - val_loss: 0.5919\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.02924\n",
            "Epoch 175/300\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.2296 - val_loss: 0.1733\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.02924\n",
            "Epoch 176/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1437 - val_loss: 0.2594\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.02924\n",
            "Epoch 177/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1071 - val_loss: 0.1071\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.02924\n",
            "Epoch 178/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0628 - val_loss: 0.1858\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.02924\n",
            "Epoch 179/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1569 - val_loss: 0.2396\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.02924\n",
            "Epoch 180/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1631 - val_loss: 0.3038\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.02924\n",
            "Epoch 181/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1346 - val_loss: 0.4318\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.02924\n",
            "Epoch 182/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1930 - val_loss: 0.8394\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.02924\n",
            "Epoch 183/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4707 - val_loss: 0.3234\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.02924\n",
            "Epoch 184/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1448 - val_loss: 0.3575\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.02924\n",
            "Epoch 185/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2301 - val_loss: 0.0937\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.02924\n",
            "Epoch 186/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2261 - val_loss: 0.3103\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.02924\n",
            "Epoch 187/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2051 - val_loss: 0.2570\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.02924\n",
            "Epoch 188/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1315 - val_loss: 0.3823\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.02924\n",
            "Epoch 189/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1564 - val_loss: 0.6390\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.02924\n",
            "Epoch 190/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1819 - val_loss: 0.1646\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.02924\n",
            "Epoch 191/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1482 - val_loss: 0.0458\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.02924\n",
            "Epoch 192/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1383 - val_loss: 0.5709\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.02924\n",
            "Epoch 193/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1966 - val_loss: 0.2669\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.02924\n",
            "Epoch 194/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.1068\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.02924\n",
            "Epoch 195/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1583 - val_loss: 0.2224\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.02924\n",
            "Epoch 196/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0900 - val_loss: 0.0987\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.02924\n",
            "Epoch 197/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1516 - val_loss: 0.2935\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.02924\n",
            "Epoch 198/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1386 - val_loss: 0.1423\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.02924\n",
            "Epoch 199/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0742 - val_loss: 0.0230\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.02924 to 0.02302, saving model to /content/drive/MyDrive/TF_stock_forecaster.ckpt\n",
            "Epoch 200/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0997 - val_loss: 0.4356\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.02302\n",
            "Epoch 201/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1819 - val_loss: 0.3880\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.02302\n",
            "Epoch 202/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1687 - val_loss: 0.5996\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.02302\n",
            "Epoch 203/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2960 - val_loss: 0.2215\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.02302\n",
            "Epoch 204/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1459 - val_loss: 0.1898\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.02302\n",
            "Epoch 205/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1540 - val_loss: 0.3448\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.02302\n",
            "Epoch 206/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1435 - val_loss: 0.6129\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.02302\n",
            "Epoch 207/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1799 - val_loss: 0.1876\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.02302\n",
            "Epoch 208/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2494 - val_loss: 0.4719\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.02302\n",
            "Epoch 209/300\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.1655 - val_loss: 0.2962\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.02302\n",
            "Epoch 210/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1443 - val_loss: 0.3378\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.02302\n",
            "Epoch 211/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2453 - val_loss: 0.3798\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.02302\n",
            "Epoch 212/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2508 - val_loss: 0.4349\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.02302\n",
            "Epoch 213/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2637 - val_loss: 0.3994\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.02302\n",
            "Epoch 214/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2984 - val_loss: 0.7272\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.02302\n",
            "Epoch 215/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3344 - val_loss: 0.3313\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.02302\n",
            "Epoch 216/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1858 - val_loss: 0.6307\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.02302\n",
            "Epoch 217/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2227 - val_loss: 0.1347\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.02302\n",
            "Epoch 218/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1430 - val_loss: 0.0290\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.02302\n",
            "Epoch 219/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1272 - val_loss: 0.1538\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.02302\n",
            "Epoch 220/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1393 - val_loss: 0.1724\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.02302\n",
            "Epoch 221/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.5300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.02302\n",
            "Epoch 222/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3093 - val_loss: 0.2534\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.02302\n",
            "Epoch 223/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1968 - val_loss: 0.5335\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.02302\n",
            "Epoch 224/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3329 - val_loss: 0.3018\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.02302\n",
            "Epoch 225/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1328 - val_loss: 0.3130\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.02302\n",
            "Epoch 226/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1214 - val_loss: 0.3636\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.02302\n",
            "Epoch 227/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1722 - val_loss: 0.2607\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.02302\n",
            "Epoch 228/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1450 - val_loss: 0.2219\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.02302\n",
            "Epoch 229/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1719 - val_loss: 0.2171\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.02302\n",
            "Epoch 230/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2509 - val_loss: 0.1562\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.02302\n",
            "Epoch 231/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1347 - val_loss: 0.3328\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.02302\n",
            "Epoch 232/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2300 - val_loss: 0.3907\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.02302\n",
            "Epoch 233/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2561 - val_loss: 0.0617\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.02302\n",
            "Epoch 234/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1332 - val_loss: 0.3167\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.02302\n",
            "Epoch 235/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2254 - val_loss: 0.4999\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.02302\n",
            "Epoch 236/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2689 - val_loss: 0.0415\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.02302\n",
            "Epoch 237/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1591 - val_loss: 0.0950\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.02302\n",
            "Epoch 238/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1921 - val_loss: 0.0940\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.02302\n",
            "Epoch 239/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2414 - val_loss: 0.2553\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.02302\n",
            "Epoch 240/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1271 - val_loss: 0.3870\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.02302\n",
            "Epoch 241/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1118 - val_loss: 0.2622\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.02302\n",
            "Epoch 242/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1054 - val_loss: 0.7795\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.02302\n",
            "Epoch 243/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.3963 - val_loss: 1.0389\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.02302\n",
            "Epoch 244/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.4540 - val_loss: 0.2562\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.02302\n",
            "Epoch 245/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1283 - val_loss: 0.1247\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.02302\n",
            "Epoch 246/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2455 - val_loss: 0.1986\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.02302\n",
            "Epoch 247/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2014 - val_loss: 0.5415\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.02302\n",
            "Epoch 248/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2237 - val_loss: 0.1386\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.02302\n",
            "Epoch 249/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2124 - val_loss: 0.7198\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.02302\n",
            "Epoch 250/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2518 - val_loss: 0.1804\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.02302\n",
            "Epoch 251/300\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.1280 - val_loss: 0.3282\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.02302\n",
            "Epoch 252/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1699 - val_loss: 0.2281\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.02302\n",
            "Epoch 253/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1654 - val_loss: 0.4602\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.02302\n",
            "Epoch 254/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2637 - val_loss: 0.4485\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.02302\n",
            "Epoch 255/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1879 - val_loss: 0.0764\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.02302\n",
            "Epoch 256/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.0997\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.02302\n",
            "Epoch 257/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1363 - val_loss: 0.1421\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.02302\n",
            "Epoch 258/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1464 - val_loss: 0.3307\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.02302\n",
            "Epoch 259/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2136 - val_loss: 0.5997\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.02302\n",
            "Epoch 260/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2644 - val_loss: 0.1423\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.02302\n",
            "Epoch 261/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1605 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.02302\n",
            "Epoch 262/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1409 - val_loss: 0.5488\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.02302\n",
            "Epoch 263/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1820 - val_loss: 0.2955\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.02302\n",
            "Epoch 264/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2198 - val_loss: 0.2159\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.02302\n",
            "Epoch 265/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.2756\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.02302\n",
            "Epoch 266/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1213 - val_loss: 0.4261\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.02302\n",
            "Epoch 267/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2296 - val_loss: 0.5427\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.02302\n",
            "Epoch 268/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2707 - val_loss: 0.5712\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.02302\n",
            "Epoch 269/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2606 - val_loss: 0.0553\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.02302\n",
            "Epoch 270/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0620 - val_loss: 0.1267\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.02302\n",
            "Epoch 271/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1268 - val_loss: 0.1504\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.02302\n",
            "Epoch 272/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1792 - val_loss: 0.0539\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.02302\n",
            "Epoch 273/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1722 - val_loss: 0.5206\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.02302\n",
            "Epoch 274/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1583 - val_loss: 0.3246\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.02302\n",
            "Epoch 275/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1569 - val_loss: 0.3082\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.02302\n",
            "Epoch 276/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2224 - val_loss: 0.1162\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.02302\n",
            "Epoch 277/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1243 - val_loss: 0.1337\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.02302\n",
            "Epoch 278/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1112 - val_loss: 0.2526\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.02302\n",
            "Epoch 279/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1534 - val_loss: 0.2518\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.02302\n",
            "Epoch 280/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2029 - val_loss: 0.2331\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.02302\n",
            "Epoch 281/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1502 - val_loss: 0.0921\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.02302\n",
            "Epoch 282/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1224 - val_loss: 0.3925\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.02302\n",
            "Epoch 283/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1664 - val_loss: 0.4642\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.02302\n",
            "Epoch 284/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2433 - val_loss: 0.2762\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.02302\n",
            "Epoch 285/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1462 - val_loss: 0.0413\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.02302\n",
            "Epoch 286/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2090 - val_loss: 0.6745\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.02302\n",
            "Epoch 287/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2496 - val_loss: 0.3268\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.02302\n",
            "Epoch 288/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2543 - val_loss: 0.4628\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.02302\n",
            "Epoch 289/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2532 - val_loss: 0.1389\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.02302\n",
            "Epoch 290/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0976 - val_loss: 0.4045\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.02302\n",
            "Epoch 291/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1645 - val_loss: 0.0797\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.02302\n",
            "Epoch 292/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1436 - val_loss: 0.2278\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.02302\n",
            "Epoch 293/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1019 - val_loss: 0.4704\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.02302\n",
            "Epoch 294/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1828 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.02302\n",
            "Epoch 295/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0888 - val_loss: 0.0449\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.02302\n",
            "Epoch 296/300\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1020 - val_loss: 0.1303\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.02302\n",
            "Epoch 297/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1481 - val_loss: 0.0623\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.02302\n",
            "Epoch 298/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1463 - val_loss: 0.3204\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.02302\n",
            "Epoch 299/300\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1097 - val_loss: 0.0972\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.02302\n",
            "Epoch 300/300\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1466 - val_loss: 0.0589\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.02302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff42e8e8910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wddNT6OMNuK-",
        "outputId": "5af3e339-2847-4025-f296-fbe47f1a0a41"
      },
      "source": [
        "model.evaluate(x=X_test, y=y_test[:-3], batch_size=8, verbose=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.340738981962204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sESfVypYQ7_z",
        "outputId": "a1759a3f-f0b1-41c1-a986-42d77684f878"
      },
      "source": [
        "pred = model.predict(X_test)\n",
        "print(len(pred), len(X_train))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "183 930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgOl75bIIWTq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}